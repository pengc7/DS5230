{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DS5230_project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMVwzs0nl0kU3yvrCrcMbdV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pengc7/DS5230/blob/main/DS5230_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWFqldLjS_my"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data1 = pd.read_csv('US_counties_COVID19_health_weather_data.csv')\n",
        "data1.shape"
      ],
      "metadata": {
        "id": "gzNYQ5QyT4Xo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#replace wrong value, encoding boolean values to float\n",
        "data1['state']= data1['state'].replace(['M'],'Mississippi')\n",
        "data1['presence_of_water_violation'] = data1['presence_of_water_violation'].astype(float)"
      ],
      "metadata": {
        "id": "zRXB-fXPT7Us"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove weather variables and other irrelevant variables\n",
        "data1.drop(data1.iloc[:,186:], axis=1, inplace=True)\n",
        "data1.drop(data1.columns[[6,7,8,9]], axis=1, inplace=True)\n",
        "\n",
        "#remove data from PR and VI, too many missing values\n",
        "data1 = data1[~data1['fips'].isin(['PR', 'VI'])]\n",
        "data1 = data1[~data1['state'].isin(['Alaska'])]\n",
        "\n",
        "#some counties share the same fips (like 'PR' for counties in Porto Rico), so groupby state, county\n",
        "data1sum = data1.groupby(['state', 'county'],as_index=False)[['cases','deaths']].sum()\n",
        "data1mean = data1.groupby(['state', 'county'],as_index=False).mean()\n",
        "data1mean.drop(['cases','deaths'],axis=1, inplace=True)\n",
        "\n",
        "data = pd.merge(data1sum, data1mean, on=['state','county'],how='inner')\n",
        "\n",
        "#get socioeconomic factors, impute missing values\n",
        "factors = data.iloc[:,4:]\n",
        "factors = factors.fillna(factors.mean())"
      ],
      "metadata": {
        "id": "fs1nIRYLw8zs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "normalization "
      ],
      "metadata": {
        "id": "CJKXYHHslK-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#normalize\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# create a scaler object\n",
        "std_scaler = StandardScaler()\n",
        "\n",
        "# fit and transform the data\n",
        "factors = std_scaler.fit_transform(factors)"
      ],
      "metadata": {
        "id": "UV3Sxq6HVsk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dimensionality reduction"
      ],
      "metadata": {
        "id": "5jN4r1dCN642"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#standard PCA\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X = factors\n",
        "sk_pca = PCA(n_components=10)\n",
        "pca_res = sk_pca.fit_transform(X)\n",
        "pca_proj = sk_pca.inverse_transform(pca_res)\n",
        "recon_error = np.linalg.norm((X-pca_proj),None)\n",
        "print(\"Reconstruction_error: \", recon_error)\n",
        "exp_var_pca = sk_pca.explained_variance_ratio_\n",
        "\n",
        "plt.bar(range(0,len(exp_var_pca)), exp_var_pca, alpha=0.5, align='center')\n",
        "plt.ylabel('Explained variance')\n",
        "plt.xlabel('Principal component')\n",
        "plt.show()\n",
        "\n",
        "plt.scatter(pca_res[:, 0], pca_res[:, 1])\n",
        "plt.xlabel('pc1')\n",
        "plt.ylabel('pc2')\n",
        "plt.show()\n",
        "\n",
        "sum(exp_var_pca)"
      ],
      "metadata": {
        "id": "pKU-IWtyYWcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#kernel PCA\n",
        "#reference: https://stackoverflow.com/questions/53556359/selecting-kernel-and-hyperparameters-for-kernel-pca-reduction\n",
        "from sklearn.decomposition import KernelPCA\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "X = factors\n",
        "\n",
        "def my_scorer(estimator, X, y=None):\n",
        "    X_reduced = estimator.transform(X)\n",
        "    X_preimage = estimator.inverse_transform(X_reduced)\n",
        "    return -1 * mean_squared_error(X, X_preimage)\n",
        "\n",
        "kpca=KernelPCA(fit_inverse_transform=True, eigen_solver=\"randomized\", random_state=100, alpha = 0.8)\n",
        "\n",
        "#gridsearch\n",
        "param_grid = [{\n",
        "        \"degree\": [2,3],\n",
        "        \"gamma\": np.linspace(1e-5, 5, 5),\n",
        "        \"kernel\":[\"sigmoid\",\"poly\",\"rbf\",\"sigmoid\",\"cosine\"]}]\n",
        "grid_search = GridSearchCV(kpca, param_grid, cv=3, scoring=my_scorer)\n",
        "grid_search.fit(X)\n",
        "grid_search.best_params_"
      ],
      "metadata": {
        "id": "6Xa1xJ46Kum4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = factors\n",
        "kpca = KernelPCA(kernel=\"cosine\", eigen_solver=\"randomized\",fit_inverse_transform=True, alpha=0.8, random_state=100)\n",
        "kpca_res = kpca.fit_transform(X)\n",
        "kpca_proj = kpca.inverse_transform(kpca_res)\n",
        "kpca_recon_error = np.linalg.norm((X-kpca_proj),None)\n",
        "print(\"Reconstruction_error: \", kpca_recon_error)\n",
        "\n",
        "exp_var_kpca = np.var(kpca_res, axis=0)\n",
        "exp_var_kpca_ratio = exp_var_kpca/np.sum(exp_var_kpca)\n",
        "plt.bar(range(0,len(exp_var_kpca_ratio[:10])), exp_var_kpca_ratio[:10], alpha=0.5, align='center')\n",
        "plt.ylabel('Explained variance')\n",
        "plt.xlabel('Principal component')\n",
        "plt.show()\n",
        "\n",
        "plt.scatter(kpca_res[:, 0], kpca_res[:, 1])\n",
        "plt.xlabel('pc1')\n",
        "plt.ylabel('pc2')\n",
        "plt.show()\n",
        "\n",
        "sum(exp_var_kpca_ratio[:10])"
      ],
      "metadata": {
        "id": "gYw_OR8MRdTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum(exp_var_kpca_ratio[:20])"
      ],
      "metadata": {
        "id": "e7xFqEcg5dlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#t-sne\n",
        "#reference: https://plotly.com/python/t-sne-and-umap-projections/\n",
        "from sklearn.manifold import TSNE\n",
        "import plotly.express as px\n",
        "\n",
        "X = kpca_res[:,:20]\n",
        "\n",
        "tsne = TSNE(n_components=2, perplexity=30.0, verbose=1, random_state=100)\n",
        "projections = tsne.fit_transform(X)\n",
        "\n",
        "fig = px.scatter(\n",
        "    projections, x=0, y=1,\n",
        ")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "iy_znjEcHDDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#install umap package\n",
        "!pip install umap-learn"
      ],
      "metadata": {
        "id": "A6DE35KTN37B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#umap, reference https://plotly.com/python/t-sne-and-umap-projections/ ,https://umap-learn.readthedocs.io/en/latest/api.html\n",
        "from umap import UMAP\n",
        "import plotly.express as px\n",
        "\n",
        "X= kpca_res[:,:20]\n",
        "umap_2d = UMAP(n_components=2, n_neighbors=5, random_state=0)\n",
        "proj_2d = umap_2d.fit_transform(X)\n",
        "fig_2d = px.scatter(\n",
        "    proj_2d, x=0, y=1,\n",
        ")\n",
        "fig_2d.show()\n",
        "\n",
        "#umap_3d = UMAP(n_components=3, init='random', random_state=0)\n",
        "#proj_3d = umap_3d.fit_transform(pca_res)\n",
        "#fig_3d = px.scatter_3d(\n",
        "#    proj_3d, x=0, y=1, z=2,\n",
        "#)\n",
        "#fig_3d.update_traces(marker_size=5)\n",
        "#fig_3d.show()"
      ],
      "metadata": {
        "id": "9islghT4Lb_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clustering"
      ],
      "metadata": {
        "id": "sK0Xb_eXhVQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#elbow choosing k\n",
        "#reference: https://towardsdatascience.com/machine-learning-algorithms-part-9-k-means-example-in-python-f2ad05ed5203\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "X = proj_2d\n",
        "sse = []\n",
        "for i in range(1, 11):\n",
        "    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
        "    kmeans.fit(X)\n",
        "    sse.append(kmeans.inertia_)\n",
        "plt.plot(range(1, 11), sse)\n",
        "plt.title('Elbow Method')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('SSE')\n",
        "plt.show()\n",
        "\n",
        "diff = [0]*(len(sse)-1)\n",
        "for i in range(len(sse)-1):\n",
        "  diff[i]=(sse[i+1]/sse[i])\n",
        "#np.where(np.array(diff)<0.8)\n",
        "diff"
      ],
      "metadata": {
        "id": "10sLTe-QN-yn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "kmeans = KMeans(n_clusters=4, init='k-means++', max_iter=300, n_init=10, random_state=100)\n",
        "labels = kmeans.fit_predict(X)\n",
        "u_labels = np.unique(labels)\n",
        "\n",
        "for i in u_labels:\n",
        "    plt.scatter(X[labels == i , 0] , X[labels == i , 1] , label = i)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "score=silhouette_score(X, labels)\n",
        "\n",
        "print(\"silhouette score\", score)"
      ],
      "metadata": {
        "id": "Mnx9SY76oFaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#spectral clustering\n",
        "from sklearn.cluster import SpectralClustering\n",
        " \n",
        "X = proj_2d\n",
        "\n",
        "specCluster = SpectralClustering(n_clusters=4,random_state=200).fit(X)\n",
        "slabels = specCluster.labels_\n",
        "s_labels = np.unique(slabels)\n",
        "for i in s_labels:\n",
        "    plt.scatter(X[slabels == i , 0] , X[slabels == i , 1] , label = i)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "sil_score = silhouette_score(X, specCluster.labels_)\n",
        "print(\"silhouette score : %f\"%(sil_score))"
      ],
      "metadata": {
        "id": "jNNLsjBBnkMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification"
      ],
      "metadata": {
        "id": "oL1lXWXmpKBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#transform to df\n",
        "df = pd.DataFrame(factors,index=data.iloc[:,4:180].index, columns=data.iloc[:,4:180].columns )"
      ],
      "metadata": {
        "id": "LRsd86nPlBat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#xgboost\n",
        "#reference: https://www.datacamp.com/community/tutorials/xgboost-in-python\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from xgboost import plot_importance\n",
        "from sklearn.metrics import roc_curve,auc,roc_auc_score\n",
        "\n",
        "X,y=df,specCluster.labels_\n",
        "data_dmatrix = xgb.DMatrix(data=factors,label=specCluster.labels_)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "xg_classifier = xgb.XGBClassifier(objective ='multi:softmax', colsample_bytree = 0.3, learning_rate = 0.1,\n",
        "                max_depth = 5, alpha = 10, n_estimators = 10)#,\n",
        "xg_classifier.fit(X_train,y_train)\n",
        "preds = xg_classifier.predict(X_test)\n",
        "rmse = accuracy_score(y_test, preds)\n",
        "print(rmse)\n",
        "\n",
        "# plot feature importance\n",
        "plot_importance(xg_classifier,max_num_features=10)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WeIxbaH5m6EQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cluster the counties\n",
        "data['cluster']=pd.Series(specCluster.labels_)\n",
        "clstr = data.groupby(['cluster'],as_index=False)[['cases','deaths','percent_fair_or_poor_health',\n",
        "                                        \"percent_smokers\",\"population_density_per_sqmi\"]].mean()\n",
        "clstr['fatality'] = clstr['deaths']/clstr['cases'] \n",
        "clstr[['cases','deaths','fatality']]                                 "
      ],
      "metadata": {
        "id": "bKZxMytD0NM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clstr['population_density_per_sqmi']"
      ],
      "metadata": {
        "id": "d9GKeoHDtKHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xx1 = \"percent_fair_or_poor_health\"\n",
        "boxplot1 = data.boxplot(column=x1,by=\"cluster\")\n",
        "plt.ylabel(x1)\n",
        "plt.title(\" \")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Sg-GpQ510037"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x2 = \"average_daily_pm2_5\"\n",
        "boxplot2 = data.boxplot(column=x2,by=\"cluster\")\n",
        "plt.ylabel(x2)\n",
        "plt.title(\" \")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8GTZmimi4e8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x3 = \"population_density_per_sqmi\"\n",
        "boxplot3 = data.boxplot(column=x3,by=\"cluster\")\n",
        "plt.ylabel(x3)\n",
        "plt.title(\" \")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PNpOSekz9myp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "geo1 = data[['state','county','cluster']]\n",
        "geo2 = data1[['state', 'county','fips']].drop_duplicates()\n",
        "geo = pd.merge(geo1, geo2, on = ['state','county'])\n",
        "geo['cluster'] = geo['cluster'].apply(str)"
      ],
      "metadata": {
        "id": "aUuCzOPHCIsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#reference: https://plotly.com/python\n",
        "from urllib.request import urlopen\n",
        "import json\n",
        "with urlopen('https://raw.githubusercontent.com/plotly/datasets/master/geojson-counties-fips.json') as response:\n",
        "    counties = json.load(response)\n",
        "\n",
        "import plotly.express as px\n",
        "\n",
        "fig = px.choropleth_mapbox(geo, geojson=counties, locations='fips', color='cluster',\n",
        "                           mapbox_style=\"carto-positron\",\n",
        "                           zoom=3, center = {\"lat\": 37.0902, \"lon\": -95.7129},\n",
        "                           labels={'cluster':'cluster'},\n",
        "                            category_orders={\"cluster\": [\"0\",\"1\",\"2\",\"3\",\"4\"]}\n",
        "                          )\n",
        "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "LSuB0G7pAqrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6gYjxjAwDJMb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}